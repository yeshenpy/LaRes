The code output should be formatted as a python code string: "```python ... ```". The return variables must be consistent with those provided in the given functions. You should neither add nor remove variables, nor modify their names. I will specifically search for the "compute_reward" and "_gripper_caging_reward" reward functions.

Please carefully consider the sub-tasks that need to be completed sequentially to achieve the current task, and determine what rewards are necessary for guiding each task.

Carefully read the logic of the code above and improve the code in three ways, each of which must include the following:

(1) Reward Components: Add or remove certain components. If there are no modifications, please provide a brief reason.
for example，add xxx reward component to encourage the agent to do xxx and apply a weight x for better xxx

(2) Reward Weights: Adjust the weight of certain reward components or change the reward coefficients. If there are no modifications, please provide a brief reason.
for example，change the reaching reward weight from 5.0 to 10.0 for better xxx

(3) Reward Calculation: Modify the reward calculation methods. If there are no modifications, please provide a brief reason.
for example，change the reaching or catching reward calculation method or add exp to xxx reward component to encourage the agent to do xxx

Finally, summarize three areas of improvement and provide valid reasons for their effectiveness.

Some helpful tips for writing the reward function code:
    (1) You may find it helpful to normalize the reward to a fixed range by applying transformations like np.exp to the overall reward or its components
    (2) If you choose to transform a reward component, then you must also introduce a temperature parameter inside the transformation function; this parameter must be a named variable in the reward function and it must not be an input variable. Each transformed reward component should have its own temperature variable
    (3) Please do not simply transform the reward components or adjust the hyperparameters. Some unnecessary reward components can be removed, while some components that may be effective for learning can be added to the final reward.
    (4) Make sure the type of each input variable is correctly specified; All the necessary information is provided in the function inputs, and "self" is neither referenced nor called.
    (5) Do not modify the conditions for determining success, proximity to the object, or object grasping, as this would compromise the evaluation criteria.
    (6) It is necessary to adjust some parameters of the existing reward function, such as scaling the reward for grasping, scaling the proximity reward, or scaling the success reward. For example "If condition: reward += 1.0 to reward +=0.5 for higher importance".
    (7) If an error occurs while calling a function, implement the desired functionality based on your understanding instead of repeatedly calling the function.
